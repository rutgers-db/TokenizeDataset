{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937535a4-710d-4b55-965b-da34284afeeb",
   "metadata": {},
   "source": [
    "# Tokenize Openwebtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b21b0-4ae3-42d2-b525-3e2d0758f4bb",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844a92fe-bc75-4cd1-bc70-8b271d6484e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset openwebtext (/research/projects/zp128/dataset_tokenizedGbt2/downloaded_dataset/openwebtext/plain_text/1.0.0/85b3ae7051d2d72e7c5fdf6dfb462603aaa26e9ed506202bf3a24d261c6c40a1)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"openwebtext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412711b1-2c76-4f1c-a4fc-29e3ac128a99",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Get Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fb199c-4fdd-4aef-98db-df3ec09cf491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_map_function at 0x7f6b2c0cd1f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpt2 Tokenizer Vocab Size:50257\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/8014 [00:00<?, ?ba/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 8014/8014 [2:33:23<00:00,  1.15s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['text', 'input_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "#show how many documents in this dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"Gpt2 Tokenizer Vocab Size:\"+ str(tokenizer.vocab_size))\n",
    "print(tokenizer.is_fast)\n",
    "\n",
    "def tokenize_map_function(items):\n",
    "    # iterate each item in a batch\n",
    "    # print(type(items[\"text\"]))\n",
    "    global tokenizer\n",
    "    return tokenizer(items[\"text\"],max_length=10000)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_map_function, batched = True)\n",
    "\n",
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8569cc-a131-4e65-beb8-c07cb1a770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this dataset for future usage\n",
    "tokenized_dataset.save_to_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ed9e65-3e06-4cc2-a536-e20f1a70e60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load back\n",
    "tokenized_dataset.load_from_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5dd2078-968b-4626-9012-502588bc5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c92f0-80e3-47bf-a3a9-f7732b94fad8",
   "metadata": {},
   "source": [
    "## Write tokenized Dataset to the binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b663f58f-7893-47d5-a237-a3a756b240c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 8014/8014 [2:10:39<00:00,  1.02ba/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "encodedFile = \"./openwebtext_gpt2.bin\"\n",
    "encodedSeqF = open(encodedFile,'wb')\n",
    "\n",
    "def write2BinaryFile(items):\n",
    "    global encodedSeqF\n",
    "#     written_bytes = b''\n",
    "    for tokens in items['input_ids']:\n",
    "        tokens_len =int(len(tokens))\n",
    "        # write its length\n",
    "#         written_bytes += tokens_len.to_bytes(4,byteorder='little',signed=True)\n",
    "#         written_bytes +=np.array(tokens,dtype=np.uint32).tobytes() \n",
    "        encodedSeqF.write(tokens_len.to_bytes(4,byteorder='little',signed=True))\n",
    "        # write the list of tokens\n",
    "        encodedSeqF.write(np.array(tokens,dtype=np.uint32).tobytes())\n",
    "#     encodedSeqF.write(written_bytes) \n",
    "\n",
    "# use map function of dataset to write each tokens into binary file\n",
    "tokenized_dataset = tokenized_dataset.map(write2BinaryFile, batched = True)\n",
    "encodedSeqF.close()\n",
    "\n",
    "\n",
    "# cnt =0 \n",
    "# for tokens in tokenized_dataset['train']['input_ids']:\n",
    "#     tokens_len =int(len(tokens))\n",
    "#     encodedSeqF.write(tokens_len.to_bytes(4,byteorder='little',signed=True))\n",
    "#     cnt += 1\n",
    "#     for i in range(0,tokens_len):\n",
    "#         encodedSeqF.write((tokens[i]).to_bytes(4,byteorder='little',signed=True))\n",
    "\n",
    "#     if cnt%10000 == 0:\n",
    "#         print(\"Current Written text :\"+str(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead7891-44d1-4176-b41f-1400f2806528",
   "metadata": {},
   "source": [
    "## Debug for tokenizedPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbf8998-3ee5-4349-9e38-f8f766bbe87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(\"./tokenized_dataset/tokenized_pile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42b332a-5405-4f71-b007-e9c3ebe09776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "1\n",
      "4\n",
      "3\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "5\n",
      "3\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# it = iter(tokenized_dataset)s\n",
    "for i in range(0,79491):\n",
    "    tmp_tokens = next(it)\n",
    "    if(len(tmp_tokens['input_ids'])<=5):\n",
    "        print(len(tmp_tokens['input_ids']))\n",
    "#     print(len(tmp_tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88c287d-b5fd-424f-a686-7f41c7381dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset glue (/common/home/zp128/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e51879-ddeb-4cb9-b003-cebff7d38590",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'skip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip\u001b[49m(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataset)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mnext\u001b[39m(it)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'skip'"
     ]
    }
   ],
   "source": [
    "dataset = dataset.skip(3)\n",
    "it = iter(dataset)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1260ecf6-0b8a-4a5d-85f8-8a7f0fab5843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "encodedFile = \"./testtest.bin\"\n",
    "encodedSeqF = open(encodedFile,'wb')\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "buffer = []\n",
    "for i in range(0, dataset.num_rows):\n",
    "    text = next(it)['sentence1']\n",
    "    tokens = tokenizer(text,max_length=100000)\n",
    "    tokens = tokens['input_ids']\n",
    "    buffer.append(len(tokens))\n",
    "    buffer.extend(tokens)\n",
    "    \n",
    "    if len(buffer) > 10000:\n",
    "        encodedSeqF.write(np.array(buffer,dtype = np.uint32).tobytes())\n",
    "        # write the list of tokens\n",
    "        buffer = []\n",
    "\n",
    "encodedSeqF.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "654b2a31-2a98-43bd-aad4-4beaec4dfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"keep-job 64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764696f-9a2b-481b-a29c-61e55fde5753",
   "metadata": {},
   "source": [
    "# Get the vocabulary of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9576b3-ac80-48a2-9be0-d58087cec3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5eef55-e74c-4577-acd0-c01021f1fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a360dd2c-581f-415c-b0de-c28015b48d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ġ', 220]]\n"
     ]
    }
   ],
   "source": [
    "keys = [[k,v] for k, v in vocab.items() if v == 220]\n",
    "print(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b1054-a031-4558-88be-530a4c10f605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
