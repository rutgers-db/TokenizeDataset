{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937535a4-710d-4b55-965b-da34284afeeb",
   "metadata": {},
   "source": [
    "# Tokenize Openwebtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b21b0-4ae3-42d2-b525-3e2d0758f4bb",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844a92fe-bc75-4cd1-bc70-8b271d6484e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset openwebtext (/research/projects/zp128/dataset_tokenizedGbt2/downloaded_dataset/openwebtext/plain_text/1.0.0/85b3ae7051d2d72e7c5fdf6dfb462603aaa26e9ed506202bf3a24d261c6c40a1)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"openwebtext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412711b1-2c76-4f1c-a4fc-29e3ac128a99",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Get Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fb199c-4fdd-4aef-98db-df3ec09cf491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_map_function at 0x7f6b2c0cd1f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gpt2 Tokenizer Vocab Size:50257\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/8014 [00:00<?, ?ba/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 8014/8014 [2:33:23<00:00,  1.15s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['text', 'input_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "#show how many documents in this dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"Gpt2 Tokenizer Vocab Size:\"+ str(tokenizer.vocab_size))\n",
    "print(tokenizer.is_fast)\n",
    "\n",
    "def tokenize_map_function(items):\n",
    "    # iterate each item in a batch\n",
    "    # print(type(items[\"text\"]))\n",
    "    global tokenizer\n",
    "    return tokenizer(items[\"text\"],max_length=10000)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_map_function, batched = True)\n",
    "\n",
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8569cc-a131-4e65-beb8-c07cb1a770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this dataset for future usage\n",
    "tokenized_dataset.save_to_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ed9e65-3e06-4cc2-a536-e20f1a70e60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 8013769\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load back\n",
    "tokenized_dataset.load_from_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5dd2078-968b-4626-9012-502588bc5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c92f0-80e3-47bf-a3a9-f7732b94fad8",
   "metadata": {},
   "source": [
    "## Write tokenized Dataset to the binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b663f58f-7893-47d5-a237-a3a756b240c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 8014/8014 [2:10:39<00:00,  1.02ba/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "encodedFile = \"./openwebtext_gpt2.bin\"\n",
    "encodedSeqF = open(encodedFile,'wb')\n",
    "\n",
    "def write2BinaryFile(items):\n",
    "    global encodedSeqF\n",
    "#     written_bytes = b''\n",
    "    for tokens in items['input_ids']:\n",
    "        tokens_len =int(len(tokens))\n",
    "        # write its length\n",
    "#         written_bytes += tokens_len.to_bytes(4,byteorder='little',signed=True)\n",
    "#         written_bytes +=np.array(tokens,dtype=np.uint32).tobytes() \n",
    "        encodedSeqF.write(tokens_len.to_bytes(4,byteorder='little',signed=True))\n",
    "        # write the list of tokens\n",
    "        encodedSeqF.write(np.array(tokens,dtype=np.uint32).tobytes())\n",
    "#     encodedSeqF.write(written_bytes) \n",
    "\n",
    "# use map function of dataset to write each tokens into binary file\n",
    "tokenized_dataset = tokenized_dataset.map(write2BinaryFile, batched = True)\n",
    "encodedSeqF.close()\n",
    "\n",
    "\n",
    "# cnt =0 \n",
    "# for tokens in tokenized_dataset['train']['input_ids']:\n",
    "#     tokens_len =int(len(tokens))\n",
    "#     encodedSeqF.write(tokens_len.to_bytes(4,byteorder='little',signed=True))\n",
    "#     cnt += 1\n",
    "#     for i in range(0,tokens_len):\n",
    "#         encodedSeqF.write((tokens[i]).to_bytes(4,byteorder='little',signed=True))\n",
    "\n",
    "#     if cnt%10000 == 0:\n",
    "#         print(\"Current Written text :\"+str(cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead7891-44d1-4176-b41f-1400f2806528",
   "metadata": {},
   "source": [
    "## Debug for tokenizedPile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbf8998-3ee5-4349-9e38-f8f766bbe87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk(\"./tokenized_dataset/tokenized_pile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b42b332a-5405-4f71-b007-e9c3ebe09776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "1\n",
      "4\n",
      "3\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "5\n",
      "3\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# it = iter(tokenized_dataset)s\n",
    "for i in range(0,79491):\n",
    "    tmp_tokens = next(it)\n",
    "    if(len(tmp_tokens['input_ids'])<=5):\n",
    "        print(len(tmp_tokens['input_ids']))\n",
    "#     print(len(tmp_tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88c287d-b5fd-424f-a686-7f41c7381dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset glue (/common/home/zp128/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e51879-ddeb-4cb9-b003-cebff7d38590",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'skip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip\u001b[49m(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataset)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mnext\u001b[39m(it)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'skip'"
     ]
    }
   ],
   "source": [
    "dataset = dataset.skip(3)\n",
    "it = iter(dataset)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1260ecf6-0b8a-4a5d-85f8-8a7f0fab5843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "encodedFile = \"./testtest.bin\"\n",
    "encodedSeqF = open(encodedFile,'wb')\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "buffer = []\n",
    "for i in range(0, dataset.num_rows):\n",
    "    text = next(it)['sentence1']\n",
    "    tokens = tokenizer(text,max_length=100000)\n",
    "    tokens = tokens['input_ids']\n",
    "    buffer.append(len(tokens))\n",
    "    buffer.extend(tokens)\n",
    "    \n",
    "    if len(buffer) > 10000:\n",
    "        encodedSeqF.write(np.array(buffer,dtype = np.uint32).tobytes())\n",
    "        # write the list of tokens\n",
    "        buffer = []\n",
    "\n",
    "encodedSeqF.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "654b2a31-2a98-43bd-aad4-4beaec4dfa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"keep-job 64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764696f-9a2b-481b-a29c-61e55fde5753",
   "metadata": {},
   "source": [
    "# Get the vocabulary of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9576b3-ac80-48a2-9be0-d58087cec3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5eef55-e74c-4577-acd0-c01021f1fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a360dd2c-581f-415c-b0de-c28015b48d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ġ', 220]]\n"
     ]
    }
   ],
   "source": [
    "keys = [[k,v] for k, v in vocab.items() if v == 220]\n",
    "print(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907b1054-a031-4558-88be-530a4c10f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(vocab.items(), key=lambda item: item[1]))\n",
    "sliced_dict = dict(list(sorted_dict.items())[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9bae347-f09a-4a6a-8289-cf9ad0d14280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, '#': 2, '$': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, '+': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '<': 27, '=': 28, '>': 29, '?': 30, '@': 31, 'A': 32, 'B': 33, 'C': 34, 'D': 35, 'E': 36, 'F': 37, 'G': 38, 'H': 39, 'I': 40, 'J': 41, 'K': 42, 'L': 43, 'M': 44, 'N': 45, 'O': 46, 'P': 47, 'Q': 48, 'R': 49, 'S': 50, 'T': 51, 'U': 52, 'V': 53, 'W': 54, 'X': 55, 'Y': 56, 'Z': 57, '[': 58, '\\\\': 59, ']': 60, '^': 61, '_': 62, '`': 63, 'a': 64, 'b': 65, 'c': 66, 'd': 67, 'e': 68, 'f': 69, 'g': 70, 'h': 71, 'i': 72, 'j': 73, 'k': 74, 'l': 75, 'm': 76, 'n': 77, 'o': 78, 'p': 79, 'q': 80, 'r': 81, 's': 82, 't': 83, 'u': 84, 'v': 85, 'w': 86, 'x': 87, 'y': 88, 'z': 89, '{': 90, '|': 91, '}': 92, '~': 93, '¡': 94, '¢': 95, '£': 96, '¤': 97, '¥': 98, '¦': 99, '§': 100, '¨': 101, '©': 102, 'ª': 103, '«': 104, '¬': 105, '®': 106, '¯': 107, '°': 108, '±': 109, '²': 110, '³': 111, '´': 112, 'µ': 113, '¶': 114, '·': 115, '¸': 116, '¹': 117, 'º': 118, '»': 119, '¼': 120, '½': 121, '¾': 122, '¿': 123, 'À': 124, 'Á': 125, 'Â': 126, 'Ã': 127, 'Ä': 128, 'Å': 129, 'Æ': 130, 'Ç': 131, 'È': 132, 'É': 133, 'Ê': 134, 'Ë': 135, 'Ì': 136, 'Í': 137, 'Î': 138, 'Ï': 139, 'Ð': 140, 'Ñ': 141, 'Ò': 142, 'Ó': 143, 'Ô': 144, 'Õ': 145, 'Ö': 146, '×': 147, 'Ø': 148, 'Ù': 149, 'Ú': 150, 'Û': 151, 'Ü': 152, 'Ý': 153, 'Þ': 154, 'ß': 155, 'à': 156, 'á': 157, 'â': 158, 'ã': 159, 'ä': 160, 'å': 161, 'æ': 162, 'ç': 163, 'è': 164, 'é': 165, 'ê': 166, 'ë': 167, 'ì': 168, 'í': 169, 'î': 170, 'ï': 171, 'ð': 172, 'ñ': 173, 'ò': 174, 'ó': 175, 'ô': 176, 'õ': 177, 'ö': 178, '÷': 179, 'ø': 180, 'ù': 181, 'ú': 182, 'û': 183, 'ü': 184, 'ý': 185, 'þ': 186, 'ÿ': 187, 'Ā': 188, 'ā': 189, 'Ă': 190, 'ă': 191, 'Ą': 192, 'ą': 193, 'Ć': 194, 'ć': 195, 'Ĉ': 196, 'ĉ': 197, 'Ċ': 198, 'ċ': 199, 'Č': 200, 'č': 201, 'Ď': 202, 'ď': 203, 'Đ': 204, 'đ': 205, 'Ē': 206, 'ē': 207, 'Ĕ': 208, 'ĕ': 209, 'Ė': 210, 'ė': 211, 'Ę': 212, 'ę': 213, 'Ě': 214, 'ě': 215, 'Ĝ': 216, 'ĝ': 217, 'Ğ': 218, 'ğ': 219, 'Ġ': 220, 'ġ': 221, 'Ģ': 222, 'ģ': 223, 'Ĥ': 224, 'ĥ': 225, 'Ħ': 226, 'ħ': 227, 'Ĩ': 228, 'ĩ': 229, 'Ī': 230, 'ī': 231, 'Ĭ': 232, 'ĭ': 233, 'Į': 234, 'į': 235, 'İ': 236, 'ı': 237, 'Ĳ': 238, 'ĳ': 239, 'Ĵ': 240, 'ĵ': 241, 'Ķ': 242, 'ķ': 243, 'ĸ': 244, 'Ĺ': 245, 'ĺ': 246, 'Ļ': 247, 'ļ': 248, 'Ľ': 249, 'ľ': 250, 'Ŀ': 251, 'ŀ': 252, 'Ł': 253, 'ł': 254, 'Ń': 255, 'Ġt': 256, 'Ġa': 257, 'he': 258, 'in': 259, 're': 260, 'on': 261, 'Ġthe': 262, 'er': 263, 'Ġs': 264, 'at': 265, 'Ġw': 266, 'Ġo': 267, 'en': 268, 'Ġc': 269, 'it': 270, 'is': 271, 'an': 272, 'or': 273, 'es': 274, 'Ġb': 275, 'ed': 276, 'Ġf': 277, 'ing': 278, 'Ġp': 279, 'ou': 280, 'Ġan': 281, 'al': 282, 'ar': 283, 'Ġto': 284, 'Ġm': 285, 'Ġof': 286, 'Ġin': 287, 'Ġd': 288, 'Ġh': 289, 'Ġand': 290, 'ic': 291, 'as': 292, 'le': 293, 'Ġth': 294, 'ion': 295, 'om': 296, 'll': 297, 'ent': 298, 'Ġn': 299, 'Ġl': 300, 'st': 301, 'Ġre': 302, 've': 303, 'Ġe': 304, 'ro': 305, 'ly': 306, 'Ġbe': 307, 'Ġg': 308, 'ĠT': 309, 'ct': 310, 'ĠS': 311, 'id': 312, 'ot': 313, 'ĠI': 314, 'ut': 315, 'et': 316, 'ĠA': 317, 'Ġis': 318, 'Ġon': 319, 'im': 320, 'am': 321, 'ow': 322, 'ay': 323, 'ad': 324, 'se': 325, 'Ġthat': 326, 'ĠC': 327, 'ig': 328, 'Ġfor': 329, 'ac': 330, 'Ġy': 331, 'ver': 332, 'ur': 333, 'Ġu': 334, 'ld': 335, 'Ġst': 336, 'ĠM': 337, \"'s\": 338, 'Ġhe': 339, 'Ġit': 340, 'ation': 341, 'ith': 342, 'ir': 343, 'ce': 344, 'Ġyou': 345, 'il': 346, 'ĠB': 347, 'Ġwh': 348, 'ol': 349, 'ĠP': 350, 'Ġwith': 351, 'Ġ1': 352, 'ter': 353, 'ch': 354, 'Ġas': 355, 'Ġwe': 356, 'Ġ(': 357, 'nd': 358, 'ill': 359, 'ĠD': 360, 'if': 361, 'Ġ2': 362, 'ag': 363, 'ers': 364, 'ke': 365, 'Ġ\"': 366, 'ĠH': 367, 'em': 368, 'Ġcon': 369, 'ĠW': 370, 'ĠR': 371, 'her': 372, 'Ġwas': 373, 'Ġr': 374, 'od': 375, 'ĠF': 376, 'ul': 377, 'ate': 378, 'Ġat': 379, 'ri': 380, 'pp': 381, 'ore': 382, 'ĠThe': 383, 'Ġse': 384, 'us': 385, 'Ġpro': 386, 'Ġha': 387, 'um': 388, 'Ġare': 389, 'Ġde': 390, 'ain': 391, 'and': 392, 'Ġor': 393, 'igh': 394, 'est': 395, 'ist': 396, 'ab': 397, 'rom': 398, 'ĠN': 399, 'th': 400, 'Ġcom': 401, 'ĠG': 402, 'un': 403, 'op': 404, '00': 405, 'ĠL': 406, 'Ġnot': 407, 'ess': 408, 'Ġex': 409, 'Ġv': 410, 'res': 411, 'ĠE': 412, 'ew': 413, 'ity': 414, 'ant': 415, 'Ġby': 416, 'el': 417, 'os': 418, 'ort': 419, 'oc': 420, 'qu': 421, 'Ġfrom': 422, 'Ġhave': 423, 'Ġsu': 424, 'ive': 425, 'ould': 426, 'Ġsh': 427, 'Ġthis': 428, 'nt': 429, 'ra': 430, 'pe': 431, 'ight': 432, 'art': 433, 'ment': 434, 'Ġal': 435, 'ust': 436, 'end': 437, '--': 438, 'all': 439, 'ĠO': 440, 'ack': 441, 'Ġch': 442, 'Ġle': 443, 'ies': 444, 'red': 445, 'ard': 446, 'âĢ': 447, 'out': 448, 'ĠJ': 449, 'Ġab': 450, 'ear': 451, 'iv': 452, 'ally': 453, 'our': 454, 'ost': 455, 'gh': 456, 'pt': 457, 'Ġpl': 458, 'ast': 459, 'Ġcan': 460, 'ak': 461, 'ome': 462, 'ud': 463, 'The': 464, 'Ġhis': 465, 'Ġdo': 466, 'Ġgo': 467, 'Ġhas': 468, 'ge': 469, \"'t\": 470, 'ĠU': 471, 'rou': 472, 'Ġsa': 473, 'Ġj': 474, 'Ġbut': 475, 'Ġwor': 476, 'Ġall': 477, 'ect': 478, 'Ġk': 479, 'ame': 480, 'Ġwill': 481, 'ok': 482, 'Ġwhe': 483, 'Ġthey': 484, 'ide': 485, '01': 486, 'ff': 487, 'ich': 488, 'pl': 489, 'ther': 490, 'Ġtr': 491, '..': 492, 'Ġint': 493, 'ie': 494, 'ure': 495, 'age': 496, 'Ġne': 497, 'ial': 498, 'ap': 499}\n"
     ]
    }
   ],
   "source": [
    "print(sliced_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af0e8e1-d4c6-4856-b41a-d963556a60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /common/home/zp128/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11df45c1-a723-4702-8c28-0fc2fcb1f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', 'Ā']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from transformers import GPT2Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define text to tokenize\n",
    "text = \"The quick brown fox jumps over the lazy dog.\\x00\"\n",
    "\n",
    "# Tokenize text and filter out stopwords and punctuation\n",
    "tokens = tokenizer.tokenize(text)\n",
    "filtered_tokens = [t for t in tokens if t not in stop_words and t not in string.punctuation]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a9b2b6-4629-4e93-b556-0cd1b51bf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.', 'Ā']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62a244b-76d2-447a-84e8-45ea156ed393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains NO Special Characters.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string_check= re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "if(string_check.search(text) == None):\n",
    "    print(\"Contains NO Special Characters.\")\n",
    "else:\n",
    "    print(\"Contains Special Characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0e96d4-a86a-4780-8fd3-f364d42d57f7",
   "metadata": {},
   "source": [
    "## Get the all the stopwords and special characters in GPT2tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41a638e-626b-4715-badd-157c826da3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# list of escaped characters\n",
    "escaped_chars = [\"\\x00\", \"\\x01\", \"\\x02\", \"\\x03\", \"\\x04\", \"\\x05\", \"\\x06\", \"\\x07\", \"\\x08\", \"\\x09\", \"\\x0A\", \"\\x0B\", \"\\x0C\", \"\\x0D\", \"\\x0E\", \"\\x0F\",\n",
    "                 \"\\x10\", \"\\x11\", \"\\x12\", \"\\x13\", \"\\x14\", \"\\x15\", \"\\x16\", \"\\x17\", \"\\x18\", \"\\x19\", \"\\x1A\", \"\\x1B\", \"\\x1C\", \"\\x1D\", \"\\x1E\", \"\\x1F\"]\n",
    "\n",
    "# list of punctuation characters\n",
    "punctuation_chars = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"\\'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\"]\n",
    "\n",
    "# set of stopwords in english\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b448ae7f-1d57-4ddf-ba6c-4ab0f818788c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 25, 26, 27, 28, 29, 30, 31, 58, 59, 60, 61, 62, 63, 77, 90, 91, 92, 93, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 256, 257, 262, 264, 267, 272, 281, 284, 285, 286, 287, 288, 290, 302, 307, 318, 319, 326, 329, 331, 338, 339, 340, 345, 351, 355, 356, 373, 379, 389, 393, 407, 416, 422, 423, 427, 428, 460, 465, 466, 468, 470, 475, 477, 481, 484, 502, 503, 508, 510, 511, 517, 523, 534, 543, 546, 547, 550, 572, 584, 587, 597, 606, 607, 611, 612, 616, 617, 618, 621, 625, 644, 645, 655, 656, 663, 673, 674, 683, 691, 703, 706, 716, 739, 749, 750, 757, 761, 777, 780, 783, 788, 810, 815, 821, 832, 836, 845, 852, 857, 866, 878, 883, 884, 898, 976, 981, 994, 1022, 1028, 1053, 1111, 1123, 1141, 1165, 1178, 1183, 1244, 1276, 1312, 1422, 1521, 1549, 1566, 1569, 1595, 1719, 1752, 1804, 1839, 2029, 2125, 2174, 2241, 2252, 2346, 2405, 2492, 3511, 3521, 3588, 3589, 3636, 4150, 4249, 4398, 5223, 5818, 6304, 6584, 6731, 8020, 12431, 16903, 17266, 18959, 22021, 25144, 27012, 32660]\n"
     ]
    }
   ],
   "source": [
    "tokenized_chars_tokens = []\n",
    "for char in escaped_chars:\n",
    "    tokenized_chars_tokens.extend(tokenizer(char)['input_ids'])\n",
    "\n",
    "# Because most of stop_words will be prefiexed by a whitespace, and that will change the tokenization, we should manually add it \n",
    "for char in stop_words:\n",
    "    tokenized_chars_tokens.extend(tokenizer(\" \"+char)['input_ids'])\n",
    "\n",
    "for char in punctuation_chars:\n",
    "    tokenized_chars_tokens.extend(tokenizer(char)['input_ids'])\n",
    "\n",
    "# eliminate duplicate elements\n",
    "tokenized_chars_tokens.extend([220]) # the whitespace\n",
    "tokenized_chars_tokens = list(set(tokenized_chars_tokens))\n",
    "tokenized_chars_tokens.sort()\n",
    "print(tokenized_chars_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58f29a4-9d49-4124-9f9c-2ff5c27a1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized_chars_tokens as a binary file\n",
    "import numpy as np\n",
    "binaryFileName = \"./filtered_tokens.bin\"\n",
    "bFile = open(binaryFileName,'wb')\n",
    "buffer = []\n",
    "buffer.append(len(tokenized_chars_tokens))\n",
    "buffer.extend(tokenized_chars_tokens)\n",
    "bFile.write(np.array(buffer,dtype = np.uint32).tobytes())\n",
    "bFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9925d81-b9ef-4a2f-9304-d0a4ff39ac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
