{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937535a4-710d-4b55-965b-da34284afeeb",
   "metadata": {},
   "source": [
    "# Tokenize Openwebtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a92fe-bc75-4cd1-bc70-8b271d6484e5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download Dataset\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"openwebtext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb199c-4fdd-4aef-98db-df3ec09cf491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer and Get Vocab Size\n",
    "\n",
    "#show how many documents in this dataset\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# You can choose another tokenizer if you want\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"Gpt2 Tokenizer Vocab Size:\"+ str(tokenizer.vocab_size))\n",
    "print(tokenizer.is_fast)\n",
    "\n",
    "def tokenize_map_function(items):\n",
    "    # iterate each item in a batch\n",
    "    # print(type(items[\"text\"]))\n",
    "    global tokenizer\n",
    "    return tokenizer(items[\"text\"],max_length=10000)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_map_function, batched = True)\n",
    "\n",
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8569cc-a131-4e65-beb8-c07cb1a770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this dataset for future usage\n",
    "tokenized_dataset.save_to_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed9e65-3e06-4cc2-a536-e20f1a70e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back\n",
    "tokenized_dataset.load_from_disk(\"./tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5dd2078-968b-4626-9012-502588bc5ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c92f0-80e3-47bf-a3a9-f7732b94fad8",
   "metadata": {},
   "source": [
    "## Write tokenized Dataset to a binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b663f58f-7893-47d5-a237-a3a756b240c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 8014/8014 [2:10:39<00:00,  1.02ba/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "encodedFile = \"./openwebtext_gpt2.bin\"\n",
    "encodedSeqF = open(encodedFile,'wb')\n",
    "\n",
    "def write2BinaryFile(items):\n",
    "    global encodedSeqF\n",
    "#     written_bytes = b''\n",
    "    for tokens in items['input_ids']:\n",
    "        tokens_len =int(len(tokens))\n",
    "        # write its length\n",
    "#         written_bytes += tokens_len.to_bytes(4,byteorder='little',signed=True)\n",
    "#         written_bytes +=np.array(tokens,dtype=np.uint32).tobytes() \n",
    "        encodedSeqF.write(tokens_len.to_bytes(4,byteorder='little',signed=True))\n",
    "        # write the list of tokens\n",
    "        encodedSeqF.write(np.array(tokens,dtype=np.uint32).tobytes())\n",
    "#     encodedSeqF.write(written_bytes) \n",
    "\n",
    "# use map function of dataset to write each tokens into binary file\n",
    "tokenized_dataset = tokenized_dataset.map(write2BinaryFile, batched = True)\n",
    "encodedSeqF.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
