{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the all the stopwords and special characters in GPT2tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/zp128/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# list of escaped characters\n",
    "escaped_chars = [\"\\x00\", \"\\x01\", \"\\x02\", \"\\x03\", \"\\x04\", \"\\x05\", \"\\x06\", \"\\x07\", \"\\x08\", \"\\x09\", \"\\x0A\", \"\\x0B\", \"\\x0C\", \"\\x0D\", \"\\x0E\", \"\\x0F\",\n",
    "                 \"\\x10\", \"\\x11\", \"\\x12\", \"\\x13\", \"\\x14\", \"\\x15\", \"\\x16\", \"\\x17\", \"\\x18\", \"\\x19\", \"\\x1A\", \"\\x1B\", \"\\x1C\", \"\\x1D\", \"\\x1E\", \"\\x1F\"]\n",
    "\n",
    "# list of punctuation characters\n",
    "punctuation_chars = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"\\'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\"]\n",
    "\n",
    "# set of stopwords in english\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 25, 26, 27, 28, 29, 30, 31, 58, 59, 60, 61, 62, 63, 77, 90, 91, 92, 93, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 256, 257, 262, 264, 267, 272, 281, 284, 285, 286, 287, 288, 290, 302, 307, 318, 319, 326, 329, 331, 338, 339, 340, 345, 351, 355, 356, 373, 379, 389, 393, 407, 416, 422, 423, 427, 428, 460, 465, 466, 468, 470, 475, 477, 481, 484, 502, 503, 508, 510, 511, 517, 523, 534, 543, 546, 547, 550, 572, 584, 587, 597, 606, 607, 611, 612, 616, 617, 618, 621, 625, 644, 645, 655, 656, 663, 673, 674, 683, 691, 703, 706, 716, 739, 749, 750, 757, 761, 777, 780, 783, 788, 810, 815, 821, 832, 836, 845, 852, 857, 866, 878, 883, 884, 898, 976, 981, 994, 1022, 1028, 1053, 1111, 1123, 1141, 1165, 1178, 1183, 1244, 1276, 1312, 1422, 1521, 1549, 1566, 1569, 1595, 1719, 1752, 1804, 1839, 2029, 2125, 2174, 2241, 2252, 2346, 2405, 2492, 3511, 3521, 3588, 3589, 3636, 4150, 4249, 4398, 5223, 5818, 6304, 6584, 6731, 8020, 12431, 16903, 17266, 18959, 22021, 25144, 27012, 32660]\n"
     ]
    }
   ],
   "source": [
    "tokenized_chars_tokens = []\n",
    "for char in escaped_chars:\n",
    "    tokenized_chars_tokens.extend(tokenizer(char)['input_ids'])\n",
    "\n",
    "# Because most of stop_words will be prefiexed by a whitespace, and that will change the tokenization, we should manually add it \n",
    "for char in stop_words:\n",
    "    tokenized_chars_tokens.extend(tokenizer(\" \"+char)['input_ids'])\n",
    "\n",
    "for char in punctuation_chars:\n",
    "    tokenized_chars_tokens.extend(tokenizer(char)['input_ids'])\n",
    "\n",
    "# eliminate duplicate elements\n",
    "tokenized_chars_tokens.extend([220]) # the whitespace\n",
    "tokenized_chars_tokens = list(set(tokenized_chars_tokens))\n",
    "tokenized_chars_tokens.sort()\n",
    "print(tokenized_chars_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized_chars_tokens as a binary file\n",
    "import numpy as np\n",
    "binaryFileName = \"./filtered_tokens.bin\"\n",
    "bFile = open(binaryFileName,'wb')\n",
    "buffer = []\n",
    "buffer.append(len(tokenized_chars_tokens))\n",
    "buffer.extend(tokenized_chars_tokens)\n",
    "bFile.write(np.array(buffer,dtype = np.uint32).tobytes())\n",
    "bFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the all the stopwords and special characters in 20B_tokenizer (A tokenizer file from RedPajama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"20B_tokenizer.json\")\n",
    "\n",
    "# list of escaped characters\n",
    "escaped_chars = [\"\\x00\", \"\\x01\", \"\\x02\", \"\\x03\", \"\\x04\", \"\\x05\", \"\\x06\", \"\\x07\", \"\\x08\", \"\\x09\", \"\\x0A\", \"\\x0B\", \"\\x0C\", \"\\x0D\", \"\\x0E\", \"\\x0F\",\n",
    "                 \"\\x10\", \"\\x11\", \"\\x12\", \"\\x13\", \"\\x14\", \"\\x15\", \"\\x16\", \"\\x17\", \"\\x18\", \"\\x19\", \"\\x1A\", \"\\x1B\", \"\\x1C\", \"\\x1D\", \"\\x1E\", \"\\x1F\"]\n",
    "\n",
    "# list of punctuation characters\n",
    "punctuation_chars = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"\\'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\"]\n",
    "\n",
    "# set of stopwords in english\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 27, 28, 29, 30, 31, 32, 33, 60, 61, 62, 63, 64, 65, 79, 92, 93, 94, 95, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 246, 247, 253, 256, 258, 266, 271, 273, 275, 277, 278, 281, 285, 294, 310, 320, 323, 326, 327, 340, 342, 344, 347, 352, 359, 368, 369, 387, 390, 403, 407, 417, 432, 434, 436, 439, 452, 476, 479, 497, 512, 513, 521, 533, 534, 556, 562, 574, 588, 594, 597, 598, 604, 616, 617, 619, 625, 626, 627, 634, 642, 643, 644, 665, 667, 670, 672, 685, 689, 690, 697, 703, 715, 717, 731, 745, 752, 760, 762, 776, 779, 816, 824, 835, 840, 841, 846, 849, 858, 875, 878, 891, 943, 949, 954, 969, 984, 1016, 1024, 1053, 1057, 1060, 1066, 1072, 1077, 1078, 1097, 1110, 1146, 1211, 1223, 1309, 1364, 1411, 1472, 1512, 1537, 1643, 1670, 1833, 1840, 1849, 1871, 1904, 1907, 1912, 1919, 2007, 2139, 2378, 2506, 2509, 2708, 2994, 3139, 3548, 3589, 3746, 4266, 4543, 4571, 4834, 5082, 5207, 5972, 6403, 6429, 6468, 8715, 9361, 10095, 10345, 11210, 13298, 16326, 20451, 22502, 26198, 31187, 36544]\n"
     ]
    }
   ],
   "source": [
    "tokenized_chars_tokens = []\n",
    "for char in escaped_chars:\n",
    "    tokenized_chars_tokens.extend(tokenizer.encode(char).ids)\n",
    "\n",
    "# Because most of stop_words will be prefiexed by a whitespace, and that will change the tokenization, we should manually add it \n",
    "for char in stop_words:\n",
    "    tokenized_chars_tokens.extend(tokenizer.encode(\" \"+char).ids)\n",
    "\n",
    "for char in punctuation_chars:\n",
    "    tokenized_chars_tokens.extend(tokenizer.encode(char).ids)\n",
    "\n",
    "# eliminate duplicate elements\n",
    "tokenized_chars_tokens.extend([209]) # the whitespace\n",
    "tokenized_chars_tokens = list(set(tokenized_chars_tokens))\n",
    "tokenized_chars_tokens.sort()\n",
    "print(tokenized_chars_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50277\n"
     ]
    }
   ],
   "source": [
    "#get some porperty of this vocab\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized_chars_tokens as a binary file\n",
    "import numpy as np\n",
    "binaryFileName = \"./20BTokenizer_filtered_tokens.bin\"\n",
    "bFile = open(binaryFileName,'wb')\n",
    "buffer = []\n",
    "buffer.append(len(tokenized_chars_tokens))\n",
    "buffer.extend(tokenized_chars_tokens)\n",
    "bFile.write(np.array(buffer,dtype = np.uint32).tobytes())\n",
    "bFile.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
